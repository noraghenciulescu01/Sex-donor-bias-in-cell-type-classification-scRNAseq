{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafffa61-4a01-4f95-9b2b-9935acb59252",
   "metadata": {},
   "source": [
    "### Create folder structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a75033c-0165-4906-8cbf-aa3ea83fcbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# folder names\n",
    "root_folder = 'classif_donorbased'\n",
    "seeds = ['seed17', 'seed42', 'seed60', 'seed83']\n",
    "anns = ['ann_2', 'ann_3', 'ann_4', 'ann_finest']\n",
    "classifs = ['knn_classif', 'rf_classif']\n",
    "result_folders = ['cms', 'norm_cms']\n",
    "\n",
    "for seed in seeds:\n",
    "    for ann in anns:\n",
    "        for classif in classifs:\n",
    "            for result_folder in result_folders:\n",
    "                path = os.path.join(root_folder, seed, ann, classif, result_folder)\n",
    "                os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f231a-aa4b-441c-a03a-14111febea2c",
   "metadata": {},
   "source": [
    "### Functions to create templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e54e34ff-377c-4cc9-9d1c-23ea8b878fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_assignment(ann_value):\n",
    "    # for 'None' entries, go back and get coarser cell type label\n",
    "    if ann_value == \"adata.obs.ann_level_3\":\n",
    "        return (\n",
    "            \"cell_type_labels = adata.obs['ann_level_3'].astype(str)\\n\"\n",
    "            \"cell_type_labels = cell_type_labels.where(cell_type_labels != 'None', adata.obs['ann_level_2'].astype(str))\"\n",
    "        )\n",
    "    elif ann_value == \"adata.obs.ann_level_4\":\n",
    "        return (\n",
    "            \"cell_type_labels = adata.obs['ann_level_4'].astype(str)\\n\"\n",
    "            \"cell_type_labels = cell_type_labels.where(cell_type_labels != 'None', adata.obs['ann_level_3'].astype(str))\\n\"\n",
    "            \"cell_type_labels = cell_type_labels.where(cell_type_labels != 'None', adata.obs['ann_level_2'].astype(str))\"\n",
    "        )\n",
    "    else:\n",
    "        # no need to handle 'None's for ann_2 and ann_finest\n",
    "        return f\"cell_type_labels = {ann_value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82832b5d-de8d-4b42-a0fe-b506fc21bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN template\n",
    "def generate_knn_script(prop_value, ann_value):\n",
    "    label_assignment = get_label_assignment(ann_value)\n",
    "    return f\"\"\"# KNN classification script\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print('successfully imported packages!', flush=True)\n",
    "\n",
    "path = '/winhome/noraghenciules/hlca_data.h5ad'\n",
    "adata = anndata.read_h5ad(path)\n",
    "\n",
    "print('successfully read file!', flush=True)\n",
    "\n",
    "embedding = adata.obsm['X_scanvi_emb']\n",
    "sex_labels = adata.obs['sex']\n",
    "individual_labels = adata.obs['donor_id']\n",
    "{label_assignment}\n",
    "classes = sorted(list(set(cell_type_labels)))\n",
    "\n",
    "from helper_functions_final import final_train_clf_and_predict, final_evaluate_clf, plot_confusion, fixed_select_indices_by_proportion, check_missing_classes_in_folds\n",
    "\n",
    "# ----------\n",
    "# Run on data increasing female proportion:\n",
    "    # (results are printed and plots are saved)\n",
    "\n",
    "prop = {prop_value}\n",
    "\n",
    "print(f\"PROPORTION OF FEMALE CELLS: {{prop}}\", flush=True)\n",
    "print('Training and testing...', flush=True)\n",
    "male_pred, male_true_labels, female_pred, female_true_labels = final_train_clf_and_predict(embedding, cell_type_labels, sex_labels, individual_labels, prop)\n",
    "print('Evaluating...', flush=True)\n",
    "male_metrics = final_evaluate_clf(male_pred, male_true_labels, classes, prop, 'male')\n",
    "female_metrics = final_evaluate_clf(female_pred, female_true_labels, classes, prop, 'female')\n",
    "\n",
    "# File path to save the dictionary\n",
    "male_file_path = f\"{{''.join(str(prop).split('.'))}}_male_metrics.pickle\"\n",
    "female_file_path = f\"{{''.join(str(prop).split('.'))}}_female_metrics.pickle\"\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open(male_file_path, 'wb') as file:\n",
    "    pickle.dump(male_metrics, file)\n",
    "\n",
    "with open(female_file_path, 'wb') as file:\n",
    "    pickle.dump(female_metrics, file)\n",
    "\"\"\"\n",
    "\n",
    "# RF template\n",
    "def generate_rf_script(prop_value, ann_value):\n",
    "    label_assignment = get_label_assignment(ann_value)\n",
    "    return f\"\"\"# RF classification script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print('successfully imported packages!', flush=True)\n",
    "\n",
    "path = '/winhome/noraghenciules/hlca_data.h5ad'\n",
    "adata = anndata.read_h5ad(path)\n",
    "\n",
    "print('successfully read file!', flush=True)\n",
    "\n",
    "counts = adata.X\n",
    "sex_labels = adata.obs['sex']\n",
    "individual_labels = adata.obs['donor_id']\n",
    "{label_assignment}\n",
    "classes = sorted(list(set(cell_type_labels)))\n",
    "\n",
    "from helper_functions import train_clf_and_predict, evaluate_clf, plot_confusion, fixed_select_indices_by_proportion, check_missing_classes_in_folds\n",
    "\n",
    "# ----------\n",
    "# Run on data increasing female proportion:\n",
    "    # (results are printed and plots are saved)\n",
    "\n",
    "prop = {prop_value}\n",
    "\n",
    "print(f\"PROPORTION OF FEMALE CELLS: {{prop}}\", flush=True)\n",
    "print('Training and testing...', flush=True)\n",
    "male_pred, male_true_labels, female_pred, female_true_labels = final_train_clf_and_predict(counts, cell_type_labels, sex_labels, individual_labels, prop, classifier = 'rf')\n",
    "print('Evaluating...', flush=True)\n",
    "male_metrics = evaluate_clf(male_pred, male_true_labels, classes, prop, 'male')\n",
    "female_metrics = evaluate_clf(female_pred, female_true_labels, classes, prop, 'female')\n",
    "\n",
    "# File path to save the dictionary\n",
    "male_file_path = f\"{{''.join(str(prop).split('.'))}}_male_metrics.pickle\"\n",
    "female_file_path = f\"{{''.join(str(prop).split('.'))}}_female_metrics.pickle\"\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open(male_file_path, 'wb') as file:\n",
    "    pickle.dump(male_metrics, file)\n",
    "\n",
    "with open(female_file_path, 'wb') as file:\n",
    "    pickle.dump(female_metrics, file)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df3b2c5-03ea-4850-a89d-f5fd506ca8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions file\n",
    "def generate_helper_file(seed_value):\n",
    "    return f\"\"\"# Helper functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GroupShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, precision_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Train and evaluate functions:\n",
    "\n",
    "def train_clf_and_predict(X, y, sex_labels, individual_labels, proportion_female, classifier='knn', k = 30):\n",
    "    '''\n",
    "    Parameters:\n",
    "    X = expression matrix; matrix of shape n_obs x n_vars\n",
    "    y = (cell type) labels; array/list of shape n_obs\n",
    "    sex_labels = 'male' or 'female' label for each entry; array/list of shape n_obs\n",
    "    individual_labels = donor id for each entry; array/list of shape n_obs\n",
    "    proportion_female = desired proportion of female cells; float between 0 and 1\n",
    "    classifier = 'knn' or 'rf'; default 'knn'\n",
    "    k = number of neighbors if using knn; default 30\n",
    "    ----------\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    male_pred, female_pred = arrays of shape n_obs; contains the prediction on the male \n",
    "                                or female test set\n",
    "    y_male_test, y_female_test = arrays of shape n_obs; contains the true labels of the \n",
    "                                male or female test set\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed({seed_value})\n",
    "    \n",
    "    \n",
    "    male_indices = np.where(sex_labels == 'male')[0]\n",
    "    female_indices = np.where(sex_labels == 'female')[0]\n",
    "\n",
    "    X_male = X[male_indices]\n",
    "    y_male = y[male_indices]\n",
    "    individual_labels_male = np.array(individual_labels)[male_indices]\n",
    "\n",
    "    X_female = X[female_indices]\n",
    "    y_female = y[female_indices]\n",
    "    individual_labels_female = np.array(individual_labels)[female_indices]\n",
    "    \n",
    "    # GroupShuffleSplit to ensure no overlap of individuals\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state={seed_value})\n",
    "\n",
    "    # Split female data\n",
    "    for train_idx, test_idx in gss.split(X_female, y_female, groups=individual_labels_female):\n",
    "        X_female_train, X_female_test = X_female[train_idx], X_female[test_idx]\n",
    "        y_female_train, y_female_test = y_female[train_idx], y_female[test_idx]\n",
    "        individual_labels_female_train = individual_labels_female[train_idx]\n",
    "        individual_labels_female_test = individual_labels_female[test_idx]\n",
    "\n",
    "    # Split male data with same test size ratio\n",
    "    for train_idx, test_idx in gss.split(X_male, y_male, groups=individual_labels_male):\n",
    "        X_male_train, X_male_test = X_male[train_idx], X_male[test_idx]\n",
    "        y_male_train, y_male_test = y_male[train_idx], y_male[test_idx]\n",
    "        individual_labels_male_train = individual_labels_male[train_idx]\n",
    "        individual_labels_male_test = individual_labels_male[test_idx]\n",
    "\n",
    "    # ensure equal sized test sets\n",
    "    # first, shuffle:\n",
    "    male_shuffle_indices = np.random.permutation(X_male_test.shape[0])\n",
    "    X_male_test = X_male_test[male_shuffle_indices]\n",
    "    y_male_test = y_male_test[male_shuffle_indices]\n",
    "    female_shuffle_indices = np.random.permutation(X_female_test.shape[0])\n",
    "    X_female_test = X_female_test[female_shuffle_indices]\n",
    "    y_female_test = y_female_test[female_shuffle_indices]\n",
    "\n",
    "    # then extract equal number of entries for both sexes:\n",
    "    min_test_size = min(X_male_test.shape[0], X_female_test.shape[0])\n",
    "    X_male_test = X_male_test[:min_test_size]\n",
    "    y_male_test = y_male_test[:min_test_size]\n",
    "    X_female_test = X_female_test[:min_test_size]\n",
    "    y_female_test = y_female_test[:min_test_size]\n",
    "\n",
    "    \n",
    "    # merge training sets back together\n",
    "    X_train = vstack([X_male_train, X_female_train])\n",
    "    y_train = np.concatenate([y_male_train, y_female_train])\n",
    "    sex_labels_train = ['male'] * X_male_train.shape[0] + ['female'] * X_female_train.shape[0]\n",
    "    ind_train = np.concatenate([individual_labels_male_train, individual_labels_female_train])\n",
    "\n",
    "    # Select female cells based on proportion_female\n",
    "    selected_indices = fixed_select_indices_by_proportion(sex_labels_train, proportion_female)\n",
    "    X_selected = X_train.tocsr()[selected_indices]\n",
    "    y_selected = y_train[selected_indices]\n",
    "\n",
    "    # Initialize classifier\n",
    "    if classifier == 'knn':\n",
    "        clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    elif classifier == 'rf':\n",
    "        clf = RandomForestClassifier(n_jobs=-1)\n",
    "        \n",
    "\n",
    "    print('initialized classif!', flush = True)\n",
    "    # Train\n",
    "    clf.fit(X_selected, y_selected)\n",
    "       \n",
    "    print('done training!', flush = True)\n",
    "    \n",
    "    # Predict\n",
    "    male_pred = clf.predict(X_male_test)\n",
    "    female_pred = clf.predict(X_female_test)\n",
    "     \n",
    "    return male_pred, y_male_test, female_pred, y_female_test\n",
    "\n",
    "\n",
    "def evaluate_clf(predictions, true_labels, classes, prop, sex):\n",
    "    '''\n",
    "    This is meant to be run separately on the male and female results of the train function.\n",
    "    ---------\n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    predictions = predictions on a test set\n",
    "    true_labels = true labels on that test set\n",
    "    classes = sorted list of classes\n",
    "    prop = proportion of female cells\n",
    "    sex = 'male' or 'female'; dneotes what test set we are working with\n",
    "        # prop and sex are only used for naming the confusion matrix plots\n",
    "    \n",
    "    ----------\n",
    "    \n",
    "    Returns:\n",
    "\n",
    "    accuracy = accuracy score\n",
    "    f1_per_class = array of shape n_classes; each entry is the f1 score of that class\n",
    "    median_f1 = median of the f1_per_class array\n",
    "    precision_per_class = array of shape n_classes; each entry is the precision score of that class\n",
    "    median_precision = median of the precision_per_class array\n",
    "    cm = confusion matrix\n",
    "    cm_normalized = normalized confusion matrix\n",
    "        (the function saves plots of each confusion matrix)\n",
    "    \n",
    "    '''\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Accuracy scores\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # F1 per class\n",
    "    f1_per_class = f1_score(true_labels, predictions, average=None)\n",
    "\n",
    "    # Median F1\n",
    "    median_f1 = np.median(f1_per_class)\n",
    "\n",
    "    # Precision per class\n",
    "    precision_per_class = precision_score(true_labels, predictions, average=None)\n",
    "\n",
    "    # Median precision\n",
    "    median_precision = np.median(precision_per_class)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=classes)\n",
    "    # Normalized confusion matrix:\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    \n",
    "    # Create dictionary\n",
    "    metrics = {{\n",
    "        'accuracy': accuracy,\n",
    "        'f1_scores': f1_per_class,\n",
    "        'median_f1': median_f1,\n",
    "        'precision_scores': precision_per_class,\n",
    "        'median_precision': median_precision,\n",
    "        'aggregated_confusion_matrix': cm,\n",
    "        'normalized_aggregated_confusion_matrix': cm_normalized\n",
    "    }}\n",
    "        \n",
    "        \n",
    "    plot_confusion(cm, classes, f'Prop {{prop}}, {{sex}} test set Confusion Matrix', False)\n",
    "    plot_confusion(cm_normalized, classes, f'Prop {{prop}}, {{sex}} test set Normalized Confusion Matrix', True)\n",
    "    \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "# Helper functions:\n",
    "\n",
    "def plot_confusion(confusion_matrix, classes, title, normalize = False):\n",
    "    '''\n",
    "    Plot and save the current confusion matrix.\n",
    "    Make sure to pass a title and the normalize parameter (if the matrix is normalized).\n",
    "    '''\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    if normalize:\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\".3f\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    else:\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\"g\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    if normalize:\n",
    "        plt.savefig(f'norm_cms/{{title}}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'cms/{{title}}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def fixed_select_indices_by_proportion(sex_labels, proportion_female):\n",
    "    np.random.seed({seed_value})\n",
    "    sex_labels_series = pd.Series( (el for el in sex_labels) )\n",
    "    \n",
    "    female_indices = np.where(sex_labels_series == 'female')[0]\n",
    "    male_indices = np.where(sex_labels_series == 'male')[0]\n",
    "    \n",
    "    fixed_size = min(len(female_indices), len(male_indices))\n",
    "    \n",
    "    np.random.shuffle(female_indices)\n",
    "    np.random.shuffle(male_indices)\n",
    "\n",
    "    num_female_cells = int(fixed_size * proportion_female)\n",
    "    num_male_cells = fixed_size - num_female_cells\n",
    "        # total will always be fixed_size\n",
    "        # this works for cases with prop 0% or 100% --> no need to handle them separately\n",
    "    \n",
    "    # adjust in case of rounding errors\n",
    "    num_female_cells = min(num_female_cells, len(female_indices))\n",
    "    num_male_cells = min(num_male_cells, len(male_indices))\n",
    "\n",
    "    selected_female_indices = female_indices[:num_female_cells]\n",
    "    selected_male_indices = male_indices[:num_male_cells]\n",
    "\n",
    "    return np.concatenate([selected_female_indices, selected_male_indices])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_missing_classes_in_folds(predictions, true_labels, classes):\n",
    "    '''\n",
    "    Checks if there are folds that miss predictions or true labels for a class.\n",
    "    The latter can happen if there are fewer samples of a certain class than folds.\n",
    "    '''\n",
    "    \n",
    "    missing_info = [] \n",
    "    \n",
    "    for fold_index, (y_pred, y_true) in enumerate(zip(predictions, true_labels)):\n",
    "        unique_pred_classes = set(np.unique(y_pred))\n",
    "        unique_true_classes = set(np.unique(y_true))\n",
    "        all_classes_set = set(classes)\n",
    "\n",
    "        missing_in_pred = all_classes_set - unique_pred_classes\n",
    "        missing_in_true = all_classes_set - unique_true_classes\n",
    "\n",
    "        if missing_in_pred or missing_in_true:\n",
    "            missing_info.append({{\n",
    "                'fold': fold_index,\n",
    "                'missing_in_predictions': sorted(list(missing_in_pred)), \n",
    "                'missing_in_true_labels': sorted(list(missing_in_true))\n",
    "            }})\n",
    "\n",
    "    if missing_info:\n",
    "        for info in missing_info:\n",
    "            print(f\"Fold {{info['fold']}} is missing predictions for classes: {{info['missing_in_predictions']}}\")\n",
    "            print(f\"Fold {{info['fold']}} is missing true labels for classes: {{info['missing_in_true_labels']}}\")\n",
    "    else:\n",
    "        print(\"All folds have predictions and true labels for all classes.\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a11eaf6e-7b39-418b-8be4-99f8b423375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM scripts\n",
    "def generate_knn_slurm_script(py_filename):\n",
    "    return f\"\"\"#!/bin/sh\n",
    "#SBATCH --partition=general   # Request partition. Default is 'general' \n",
    "#SBATCH --qos=short           # Request Quality of Service. Default is 'short' (maximum run time: 4 hours)\n",
    "#SBATCH --time=0:40:00        # Request run time (wall-clock). Default is 1 minute\n",
    "#SBATCH --ntasks=1            # Request number of parallel tasks per job. Default is 1\n",
    "#SBATCH --cpus-per-task=2     # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2).\n",
    "#SBATCH --mem=20GB             # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead\n",
    "#SBATCH --mail-type=FAIL       # Set mail type to 'END' to receive a mail when the job finishes. \n",
    "#SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId\n",
    "#SBATCH --error=slurm_%j.err  # Set name of error log. %j is the Slurm jobId\n",
    "\n",
    "# which python 1>&2  # Write path to Python binary to standard error\n",
    "# python --version   # Write Python version to standard error\n",
    "\n",
    "srun python3 {py_filename}\n",
    "\"\"\"\n",
    "\n",
    "# rf needs more memory + time\n",
    "def generate_rf_slurm_script(py_filename):\n",
    "    return f\"\"\"#!/bin/sh\n",
    "#SBATCH --partition=general   # Request partition. Default is 'general' \n",
    "#SBATCH --qos=short           # Request Quality of Service. Default is 'short' (maximum run time: 4 hours)\n",
    "#SBATCH --time=2:00:00        # Request run time (wall-clock). Default is 1 minute\n",
    "#SBATCH --ntasks=1            # Request number of parallel tasks per job. Default is 1\n",
    "#SBATCH --cpus-per-task=2     # Request number of CPUs (threads) per task. Default is 1 (note: CPUs are always allocated to jobs per 2).\n",
    "#SBATCH --mem=80GB             # Request memory (MB) per node. Default is 1024MB (1GB). For multiple tasks, specify --mem-per-cpu instead\n",
    "#SBATCH --mail-type=FAIL       # Set mail type to 'END' to receive a mail when the job finishes. \n",
    "#SBATCH --output=slurm_%j.out # Set name of output log. %j is the Slurm jobId\n",
    "#SBATCH --error=slurm_%j.err  # Set name of error log. %j is the Slurm jobId\n",
    "\n",
    "# which python 1>&2  # Write path to Python binary to standard error\n",
    "# python --version   # Write Python version to standard error\n",
    "\n",
    "srun python3 {py_filename}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66c97a-04d6-4ad2-9eab-1315c5027b22",
   "metadata": {},
   "source": [
    "### Write classification files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c98f6f8d-58f7-456d-b685-769d2ccf4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# dictionaries for notations:\n",
    "ann_to_value = {\n",
    "    'ann_2': \"adata.obs.ann_level_2\",\n",
    "    'ann_3': \"adata.obs.ann_level_3\",\n",
    "    'ann_4': \"adata.obs.ann_level_4\",\n",
    "    'ann_finest': \"adata.obs['ann_finest_level'].astype(str)\"\n",
    "}\n",
    "\n",
    "prop_to_str = {\n",
    "    0.0: \"0\", 0.1: \"01\", 0.2: \"02\", 0.3: \"03\", 0.4: \"04\", 0.5: \"05\",\n",
    "    0.6: \"06\", 0.7: \"07\", 0.8: \"08\", 0.9: \"09\", 1.0: \"1\"\n",
    "}\n",
    "\n",
    "seed_to_value = {\n",
    "    'seed17': 17,\n",
    "    'seed42' : 42,\n",
    "    'seed60' : 60,\n",
    "    'seed83' : 83\n",
    "}\n",
    "\n",
    "# loop through folder structure\n",
    "for seed in seeds:\n",
    "    for ann in anns:\n",
    "        for classif in classifs:\n",
    "            # helper file\n",
    "            helper_file_path = os.path.join(root_folder, seed, ann, classif, 'helper_functions.py')\n",
    "            seed_value = seed_to_value[seed]\n",
    "            with open(helper_file_path, 'w') as f:\n",
    "                    f.write(generate_helper_file(seed_value))\n",
    "            \n",
    "            # classification files\n",
    "            for prop in props:\n",
    "                prop_str = prop_to_str[prop]\n",
    "                filename = f\"{classif.split('_')[0]}_{prop_str}.py\"\n",
    "                file_path = os.path.join(root_folder, seed, ann, classif, filename)\n",
    "\n",
    "                sh_filename = f\"{classif.split('_')[0]}_{prop_str}.sh\"\n",
    "                sh_path = os.path.join(root_folder, seed, ann, classif, sh_filename)\n",
    "\n",
    "                \n",
    "                ann_value = ann_to_value[ann]\n",
    "                # write files\n",
    "                if classif == 'knn_classif':\n",
    "                    ## write SLURM file\n",
    "                    with open(sh_path, 'w', newline='\\n') as f_sh:\n",
    "                        f_sh.write(generate_knn_slurm_script(filename))\n",
    "                    \n",
    "                    ## write .py file\n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(generate_knn_script(prop, ann_value))\n",
    "\n",
    "                elif classif == 'rf_classif':\n",
    "                    ## write SLURM file\n",
    "                    with open(sh_path, 'w', newline='\\n') as f_sh:\n",
    "                        f_sh.write(generate_rf_slurm_script(filename))\n",
    "                    \n",
    "                    ## write .py file\n",
    "                    with open(file_path, 'w') as f:\n",
    "                        f.write(generate_rf_script(prop, ann_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
